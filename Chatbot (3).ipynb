{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eEQT0C5Q7YtR",
    "outputId": "6f442bf1-e8e6-432d-85ca-dc320be52c8f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\panna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\panna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\panna\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalnum() and token not in stop_words]\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Ad7H23lI7nqk"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "faq_data = {\n",
    "    \"What is your return policy?\": \"Our return policy allows returns within 30 days of purchase.\",\n",
    "    \"How long does shipping take?\": \"Shipping typically takes 3-5 business days.\",\n",
    "    \"What payment methods do you accept?\": \"We accept all major credit cards and PayPal.\"\n",
    "}\n",
    "\n",
    "# Fit and transform FAQs\n",
    "faq_questions = list(faq_data.keys())\n",
    "faq_tfidf = tfidf_vectorizer.fit_transform(faq_questions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A78vw0s-7zhE",
    "outputId": "c84ae1aa-d7f8-4a6f-d523-933d713bfe60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shipping typically takes 3-5 business days.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_best_faq_match(user_input, faq_tfidf, faq_questions):\n",
    "    user_tfidf = tfidf_vectorizer.transform([user_input])\n",
    "    similarities = cosine_similarity(user_tfidf, faq_tfidf)\n",
    "    best_match_idx = similarities.argmax()\n",
    "    return faq_questions[best_match_idx]\n",
    "\n",
    "user_input = \"How long does shipping take?\"\n",
    "best_faq = get_best_faq_match(user_input, faq_tfidf, faq_questions)\n",
    "print(faq_data[best_faq])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformersNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading transformers-4.45.2-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\panna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (3.16.1)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Downloading huggingface_hub-0.25.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\panna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\panna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\panna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\panna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\panna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.5-cp312-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers)\n",
      "  Downloading tokenizers-0.20.1-cp312-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\panna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\panna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\panna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\panna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\panna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\panna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\panna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\panna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Downloading transformers-4.45.2-py3-none-any.whl (9.9 MB)\n",
      "   ---------------------------------------- 0.0/9.9 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 1.3/9.9 MB 6.7 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 2.6/9.9 MB 7.2 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 4.5/9.9 MB 7.7 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 6.0/9.9 MB 7.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 8.1/9.9 MB 8.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.9/9.9 MB 8.1 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.25.2-py3-none-any.whl (436 kB)\n",
      "Downloading safetensors-0.4.5-cp312-none-win_amd64.whl (286 kB)\n",
      "Downloading tokenizers-0.20.1-cp312-none-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   -------------------------- ------------- 1.6/2.4 MB 7.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 8.5 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.25.2 safetensors-0.4.5 tokenizers-0.20.1 transformers-4.45.2\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf-keras\n",
      "  Downloading tf_keras-2.17.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: tensorflow<2.18,>=2.17 in c:\\users\\panna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tf-keras) (2.17.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.17.0 in c:\\users\\panna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow<2.18,>=2.17->tf-keras) (2.17.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\panna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\panna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\panna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\panna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\panna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\panna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\panna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in c:\\users\\panna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (0.4.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\panna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\panna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\panna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (4.25.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\panna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\panna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\panna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\panna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\panna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\panna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\panna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (1.66.1)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in c:\\users\\panna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (2.17.1)\n",
      "Requirement already satisfied: keras>=3.2.0 in c:\\users\\panna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (3.5.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in c:\\users\\panna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\panna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\users\\panna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (13.8.1)\n",
      "Requirement already satisfied: namex in c:\\users\\panna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\panna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (0.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\panna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\panna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\panna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\panna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\panna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\panna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\panna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (3.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\panna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\panna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\panna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\panna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (0.1.2)\n",
      "Downloading tf_keras-2.17.0-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ------------------------------ --------- 1.3/1.7 MB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 7.2 MB/s eta 0:00:00\n",
      "Installing collected packages: tf-keras\n",
      "Successfully installed tf-keras-2.17.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NYFL07rd8PDD",
    "outputId": "5fa0bcdf-a9b7-4b91-968f-ccd82df53d58"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\panna\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "355f2e5cd78549288f8a314ffb317c3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d12b01c0a004183a117c45dc7a4505c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4982b929d49c45beb1fac729242c9d1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f61e99f3c7db4d01adcd01bcf9fcc868",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-5 business days\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load a pre-trained BERT model for question answering\n",
    "question_answering = pipeline(\"question-answering\")\n",
    "\n",
    "def bert_faq_response(user_input, faq_data):\n",
    "    context = \" \".join(faq_data.values())  # Concatenate all FAQ answers as context\n",
    "    response = question_answering(question=user_input, context=context)\n",
    "    return response['answer']\n",
    "\n",
    "response = bert_faq_response(\"How long does shipping take?\", faq_data)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "GJ90vWkI8XpB"
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "eYbkA_K89D9b"
   },
   "outputs": [],
   "source": [
    "faq_data = {\n",
    "    \"What is your return policy?\": \"Our return policy allows returns within 30 days of purchase.\",\n",
    "    \"How long does shipping take?\": \"Shipping typically takes 3-5 business days.\",\n",
    "    \"What payment methods do you accept?\": \"We accept all major credit cards and PayPal.\",\n",
    "    \"Do you ship internationally?\": \"Yes, we ship to over 200 countries worldwide.\",\n",
    "    \"Can I track my order?\": \"Yes, once your order is shipped, we will provide you with a tracking number.\",\n",
    "    \"What is your returns policy?\": \"Our return policy allows returns within 30 days of purchase.\",  # Duplicate/variant\n",
    "    \"How fast is shipping?\": \"Shipping typically takes 3-5 business days.\",  # Duplicate/variant\n",
    "    \"Which cards do you accept?\": \"We accept all major credit cards and PayPal.\",  # Duplicate/variant\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QPOv5qK99IMU",
    "outputId": "054237e0-06c7-4e26-ca12-fb9b30ec8ecc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "\n",
      "Classification Report:\n",
      "                                                               precision    recall  f1-score   support\n",
      "\n",
      "Our return policy allows returns within 30 days of purchase.       1.00      1.00      1.00         1\n",
      "                 Shipping typically takes 3-5 business days.       1.00      1.00      1.00         1\n",
      "\n",
      "                                                    accuracy                           1.00         2\n",
      "                                                   macro avg       1.00      1.00      1.00         2\n",
      "                                                weighted avg       1.00      1.00      1.00         2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Prepare the data\n",
    "faq_questions = list(faq_data.keys())  # Questions are the input\n",
    "faq_answers = list(faq_data.values())  # Answers are the labels\n",
    "\n",
    "# Split the data into train and test sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(faq_questions, faq_answers, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert text data to TF-IDF vectors\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Train a Logistic Regression classifier\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_path = 'Chatbot.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(notebook_path, 'r', encoding='utf-8') as file:\n",
    "    notebook = nbformat.read(file, as_version=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'cell_type': 'code',\n",
       "  'execution_count': 1,\n",
       "  'metadata': {'colab': {'base_uri': 'https://localhost:8080/'},\n",
       "   'id': 'eEQT0C5Q7YtR',\n",
       "   'outputId': '6f442bf1-e8e6-432d-85ca-dc320be52c8f'},\n",
       "  'outputs': [{'name': 'stderr',\n",
       "    'output_type': 'stream',\n",
       "    'text': '[nltk_data] Downloading package punkt to\\n[nltk_data]     C:\\\\Users\\\\panna\\\\AppData\\\\Roaming\\\\nltk_data...\\n[nltk_data]   Unzipping tokenizers\\\\punkt.zip.\\n[nltk_data] Downloading package stopwords to\\n[nltk_data]     C:\\\\Users\\\\panna\\\\AppData\\\\Roaming\\\\nltk_data...\\n[nltk_data]   Unzipping corpora\\\\stopwords.zip.\\n[nltk_data] Downloading package wordnet to\\n[nltk_data]     C:\\\\Users\\\\panna\\\\AppData\\\\Roaming\\\\nltk_data...\\n'}],\n",
       "  'source': \"import nltk\\nfrom nltk.corpus import stopwords\\nfrom nltk.tokenize import word_tokenize\\nfrom nltk.stem import WordNetLemmatizer\\n\\nnltk.download('punkt')\\nnltk.download('stopwords')\\nnltk.download('wordnet')\\n\\nstop_words = set(stopwords.words('english'))\\nlemmatizer = WordNetLemmatizer()\\n\\ndef preprocess_text(text):\\n    tokens = word_tokenize(text.lower())\\n    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalnum() and token not in stop_words]\\n    return tokens\\n\"},\n",
       " {'cell_type': 'code',\n",
       "  'execution_count': 2,\n",
       "  'metadata': {'id': 'Ad7H23lI7nqk'},\n",
       "  'outputs': [],\n",
       "  'source': 'from sklearn.feature_extraction.text import TfidfVectorizer\\n\\ntfidf_vectorizer = TfidfVectorizer()\\nfaq_data = {\\n    \"What is your return policy?\": \"Our return policy allows returns within 30 days of purchase.\",\\n    \"How long does shipping take?\": \"Shipping typically takes 3-5 business days.\",\\n    \"What payment methods do you accept?\": \"We accept all major credit cards and PayPal.\"\\n}\\n\\n# Fit and transform FAQs\\nfaq_questions = list(faq_data.keys())\\nfaq_tfidf = tfidf_vectorizer.fit_transform(faq_questions)\\n'},\n",
       " {'cell_type': 'code',\n",
       "  'execution_count': 3,\n",
       "  'metadata': {'colab': {'base_uri': 'https://localhost:8080/'},\n",
       "   'id': 'A78vw0s-7zhE',\n",
       "   'outputId': 'c84ae1aa-d7f8-4a6f-d523-933d713bfe60'},\n",
       "  'outputs': [{'name': 'stdout',\n",
       "    'output_type': 'stream',\n",
       "    'text': 'Shipping typically takes 3-5 business days.\\n'}],\n",
       "  'source': 'from sklearn.metrics.pairwise import cosine_similarity\\n\\ndef get_best_faq_match(user_input, faq_tfidf, faq_questions):\\n    user_tfidf = tfidf_vectorizer.transform([user_input])\\n    similarities = cosine_similarity(user_tfidf, faq_tfidf)\\n    best_match_idx = similarities.argmax()\\n    return faq_questions[best_match_idx]\\n\\nuser_input = \"How long does shipping take?\"\\nbest_faq = get_best_faq_match(user_input, faq_tfidf, faq_questions)\\nprint(faq_data[best_faq])\\n'}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notebook.cells[:3]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_context = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context(user_id):\n",
    "    \"\"\"Retrieve user context if available\"\"\"\n",
    "    return conversation_context.get(user_id, None)\n",
    "\n",
    "def update_context(user_id, question):\n",
    "    \"\"\"Update the context with the latest user question\"\"\"\n",
    "    conversation_context[user_id] = question\n",
    "\n",
    "def get_best_faq_match(user_input, faq_tfidf, faq_questions, user_id=None):\n",
    "    \"\"\"Find the best FAQ match with context memory\"\"\"\n",
    "    if user_id:\n",
    "        context = get_context(user_id)\n",
    "        if context:\n",
    "            user_input = context + \" \" + user_input\n",
    "    user_tfidf = tfidf_vectorizer.transform([user_input])\n",
    "    similarities = cosine_similarity(user_tfidf, faq_tfidf)\n",
    "    best_match_idx = similarities.argmax()\n",
    "\n",
    "    if user_id:\n",
    "        update_context(user_id, user_input)  \n",
    "    \n",
    "    return faq_questions[best_match_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_synonyms(word):\n",
    "    \"\"\"Return a list of synonyms for a given word using WordNet\"\"\"\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    return synonyms\n",
    "\n",
    "def preprocess_text_with_synonyms(text):\n",
    "    \"\"\"Preprocess text and replace words with their synonyms\"\"\"\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalnum() and token not in stop_words]\n",
    "    \n",
    "    \n",
    "    tokens_with_synonyms = []\n",
    "    for token in tokens:\n",
    "        synonyms = get_synonyms(token)\n",
    "        if synonyms:\n",
    "            \n",
    "            tokens_with_synonyms.append(list(synonyms)[0])\n",
    "        else:\n",
    "            tokens_with_synonyms.append(token)\n",
    "    \n",
    "    return tokens_with_synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblobNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading textblob-0.18.0.post0-py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: nltk>=3.8 in c:\\users\\panna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from textblob) (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\panna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk>=3.8->textblob) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\panna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk>=3.8->textblob) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\panna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk>=3.8->textblob) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\panna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk>=3.8->textblob) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\panna\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from click->nltk>=3.8->textblob) (0.4.6)\n",
      "Downloading textblob-0.18.0.post0-py3-none-any.whl (626 kB)\n",
      "   ---------------------------------------- 0.0/626.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 626.3/626.3 kB 3.9 MB/s eta 0:00:00\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.18.0.post0\n"
     ]
    }
   ],
   "source": [
    "pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def correct_spelling(text):\n",
    "    \"\"\"Correct spelling in the input text using TextBlob\"\"\"\n",
    "    return str(TextBlob(text).correct())\n",
    "\n",
    "def preprocess_text_with_correction(text):\n",
    "    \"\"\"Preprocess text with spell correction\"\"\"\n",
    "    corrected_text = correct_spelling(text)\n",
    "    tokens = word_tokenize(corrected_text.lower())\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalnum() and token not in stop_words]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_multi_intents(user_input):\n",
    "    \"\"\"Split and handle multiple intents in the user input\"\"\"\n",
    "    # Split the input into multiple questions (for simplicity, use periods as delimiters)\n",
    "    intents = user_input.split('.')\n",
    "    responses = []\n",
    "    for intent in intents:\n",
    "        if intent.strip():\n",
    "            best_faq = get_best_faq_match(intent.strip(), faq_tfidf, faq_questions)\n",
    "            responses.append(faq_data[best_faq])\n",
    "    \n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shipping typically takes 3-5 business days.\n"
     ]
    }
   ],
   "source": [
    "# Ensure you're using the same TfidfVectorizer for both FAQ and user input\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Train on FAQ questions\n",
    "faq_questions = list(faq_data.keys())\n",
    "faq_tfidf = tfidf_vectorizer.fit_transform(faq_questions)\n",
    "\n",
    "# When a user asks a question, use the same vectorizer to transform their input\n",
    "def get_best_faq_match(user_input, faq_tfidf, faq_questions):\n",
    "    # Transform the user input using the same vectorizer\n",
    "    user_tfidf = tfidf_vectorizer.transform([user_input])\n",
    "    \n",
    "    # Now compute cosine similarity\n",
    "    similarities = cosine_similarity(user_tfidf, faq_tfidf)\n",
    "    best_match_idx = similarities.argmax()\n",
    "    \n",
    "    return faq_questions[best_match_idx]\n",
    "\n",
    "# Example user input\n",
    "user_input = \"How long does shipping take?\"\n",
    "best_faq = get_best_faq_match(user_input, faq_tfidf, faq_questions)\n",
    "print(faq_data[best_faq])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shipping typically takes 3-5 business days.\n"
     ]
    }
   ],
   "source": [
    "user_input = \"Can I return this item? How long does shipping take?\"\n",
    "responses = handle_multi_intents(user_input)\n",
    "for response in responses:\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shipping typically takes 3-5 business days.\n",
      "User Input: How long does shipping take?\n",
      "Chatbot Response: Shipping typically takes 3-5 business days.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Was this answer helpful? (yes/no):  yes\n"
     ]
    }
   ],
   "source": [
    "feedback_data = []\n",
    "\n",
    "def get_user_feedback(user_input, faq_answer):\n",
    "    \"\"\"Ask for user feedback after providing an answer\"\"\"\n",
    "    print(f\"User Input: {user_input}\")\n",
    "    print(f\"Chatbot Response: {faq_answer}\")\n",
    "    feedback = input(\"Was this answer helpful? (yes/no): \").lower()\n",
    "    feedback_data.append({\"user_input\": user_input, \"response\": faq_answer, \"feedback\": feedback})\n",
    "\n",
    "# Example usage\n",
    "user_input = \"How long does shipping take?\"\n",
    "best_faq = get_best_faq_match(user_input, faq_tfidf, faq_questions)\n",
    "print(faq_data[best_faq])\n",
    "get_user_feedback(user_input, faq_data[best_faq])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
